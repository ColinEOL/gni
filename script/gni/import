#!/usr/bin/env python
import os
import sys
from urllib import urlopen
import libxml2
import MySQLdb
import yaml
import marshal
import sha
from optparse import OptionParser

opts = OptionParser()
opts.add_option("-e", "--environment", dest="environment", default="development",
                  help="Specifies the environment of the system (development|test|producton).")

opts.add_option("-d", "--data-url", dest="data_url",
                  help="Specifies url which contains data for harvesting.")

opts.add_option("-s", "--source-id", dest="source_id",
                  help="Identifier of the data_source in GNA database.")

(options, args) = opts.parse_args()

if not (options.data_url and options.source_id and type(int(options.source_id)) == type(1)):
    raise Exception("DATA_URL and SOURCE_ID has to be entered")

RECORD_TEMPLATE = {"data_source_id": options.source_id}
    

def connectDb():

    db_data = os.popen('erb ' + sys.path[0] + '/../../config/database.yml').read()
    db_conf =  yaml.load(db_data)[options.environment]
    if not db_conf.has_key('socket'):
        db_conf['socket'] = "/tmp/mysql.sock"

    try:
        conn = MySQLdb.connect (
            host = db_conf['host'],
            user = db_conf['username'],
            passwd = db_conf['password'],
            unix_socket = db_conf['socket'],
            db = db_conf['database'])
    except MySQLdb.Error, e:
        print "Error %d: %s" % (e.args[0], e.args[1])
        sys.exit (1)
    return conn

def prepare_record(c, record):
    c.execute("select id from name_strings where name = '%s'" % record["raw_name"])
    name_string_id = c.fetchone()
    if not name_string_id:
        c.execute("insert into name_strings (name) values ('%s')" % record["raw_name"])
        c.execute("select last_insert_id()")
        name_string_id = c.fetchone()
    return ( name_string_id[0], record["uri"].strip())    

def insert_records(c, imported_data):
    c.execute("select name_string_id from name_indices where data_source_id = %s" % options.source_id)
    old_ids = set(map(lambda x: x[0], c.fetchall()))
    new_ids = set(imported_data.keys())
    to_delete = old_ids.difference(new_ids)
    to_insert = new_ids.difference(old_ids)
    to_check = old_ids.intersection(new_ids)
    if len(to_delete):
        delete_ids = ",".join(map(lambda x: str(x), to_delete))
        c.execute("delete from name_indices where data_source_id = %s and name_string_id in (%s)" % (options.source_id, delete_ids))
    if len(to_insert):
        inserts = []
        for i in to_insert:
            inserts.append("(%s , %s, '%s')" % (options.source_id, i, MySQLdb.escape_string(imported_data[i][0])))
        c.execute("insert into name_indices (data_source_id, name_string_id, uri) values %s" % ",".join(inserts))
    
    update_records(c, to_check, imported_data)
    
    print "deleted: " + str(len(to_delete)), "inserted: " + str(len(to_insert)), "checked for updates: " + str(len(to_check))

def update_records(c, to_check, imported_data):
    to_check = list(to_check)
    to_check.sort()
    slice_size = min_slice = 128
    max_slice = len(to_check)/20
    while len(to_check):
        slice = to_check[0:slice_size]
        print slice_size
        lookup_ids = []
        new_data = []
        for i in slice:
            new_data.append((imported_data[i][0],))
            lookup_ids.append(i)


        c.execute("select uri from name_indices where data_source_id = %s and name_string_id in (%s)" % (options.source_id,  ",".join(map(lambda x: str(x),lookup_ids))))
        if sha.new(marshal.dumps(c.fetchall())).digest() == sha.new(marshal.dumps(tuple(new_data))).digest():
            to_check = to_check[slice_size:]
            if slice_size * 2 < max_slice:
                slice_size *= 2
        elif slice_size > min_slice:
            slice_size /= 2
        else:
            for i in slice:
                new_data = imported_data[i][0]
                c.execute("select uri from name_indices where data_source_id = %s and name_string_id = %s", (options.source_id, i))
                res = c.fetchone()
                if new_data != res[0][0]:
                    c.execute("update name_indices set uri = %s where data_source_id = %s and name_string_id = %s", (new_data, options.source_id, i))
            to_check = to_check[slice_size:]
    print "done"
    

def processNode(c, reader, current_var, record, imported_data):
    if reader.NodeType() == 1:
        if reader.Name() == "dwc:ScientificName":
            current_var = "raw_name"    
        elif reader.Name() == "dc:source":
            current_var = "uri"
    elif reader.NodeType() == 15:
        if reader.Name() == "record":
            res = (prepare_record(c, record))
            imported_data[res[0]] = res[1:]
            record = RECORD_TEMPLATE
    elif reader.NodeType() == 3 and current_var:
        record[current_var] = reader.Value()
        current_var = None
    return (current_var, record)
    
def streamFile(filename):
    try:
        reader = libxml2.newTextReaderFilename(filename)
    except:
        print "cannot open %s" % (filename)
        return
    db_connection = connectDb()
    c = db_connection.cursor()
    imported_data = {}
    current_var = None
    record = RECORD_TEMPLATE      
    ret = reader.Read()

    while ret == 1:
        current_var, record = processNode(c, reader, current_var, record, imported_data)
        ret = reader.Read()

    if ret != 0:
        print "%s : failed to parse" % (filename)
        sys.exit(0)
    insert_records(c, imported_data)
    db_connection.commit()

#f = 'http://betula.mbl.edu/index_fungorum_test/data.xml'
#f = sys.path[0] + '/../spec/fixtures/feeds/index_fungorum_short.xml'

streamFile(options.data_url)
