#!/usr/bin/env python
import os
import sys
from urllib import urlopen
import libxml2
import MySQLdb
import yaml
import marshal
import sha
from optparse import OptionParser
    
import pprint
pp = pprint.PrettyPrinter(indent=2)

opts = OptionParser()
opts.add_option("-e", "--environment", dest="environment", default="development",
                  help="Specifies the environment of the system (development|test|producton).")

opts.add_option("-s", "--source", dest="source",
                  help="Specifies uri/filename which contains data for harvesting.")

opts.add_option("-i", "--source-id", dest="source_id",
                  help="Identifier of the data_source in GNA database.")

(options, args) = opts.parse_args()

if not (options.source and options.source_id and type(int(options.source_id)) == type(1)):
    raise Exception("source file/uri and source id are required")


class DbImporter: #{{{1

  def escape_data(self,data): #{{{2
    for key in data.keys():
      if data[key]:
        if type(data[key]) == type(''):
          data[key] = "'" + MySQLdb.escape_string(str(data[key])) + "'"
      else:
        data[key] = 'null'
      for key in ('uri','local_id','global_id'):
        if not data.has_key(key):
          data[key] = 'null'
    return data

  def __init__(self): #{{{2
    self.conn = self._connect()
    self.cursor = self.conn.cursor() 

  def _connect(self):
    db_data = os.popen('erb ' + sys.path[0] + '/../../config/database.yml').read()
    db_conf =  yaml.load(db_data)[options.environment]
    if not db_conf.has_key('socket'):
        db_conf['socket'] = "/tmp/mysql.sock"

    try:
        conn = MySQLdb.connect (
            host = db_conf['host'],
            user = db_conf['username'],
            passwd = db_conf['password'],
            unix_socket = db_conf['socket'],
            db = db_conf['database'])
    except MySQLdb.Error, e:
        print "Error %d: %s" % (e.args[0], e.args[1])
        sys.exit (1)
    return conn


class Importer: #{{{1

  def __init__(self): #{{{2
    self.db = DbImporter()
    self.duplicates = []
    self.deleted = []
    self.inserted = []
    self.changed = []
    self.imported_data = {}

    self.reader = libxml2.newTextReaderFilename(options.source)
    
    self._current_tag = None
    self._record = self._reset_record()

  def parse(self): #{{{2
    ret = self.reader.Read()
    while ret == 1:
      self._process_node()
      ret = self.reader.Read()
    if ret != 0:
        print "%s : failed to parse" % (filename)
        sys.exit(0)

    c = self.db.cursor
    c.execute("select name_string_id from name_indices where data_source_id = %s" % options.source_id)
    self._old_ids = set(map(lambda x: x[0], c.fetchall()))
    self._new_ids = set(self.imported_data.keys())

  def get_deleted(self): #{{{2
    try: 
      self.deleted = self._old_ids.difference(self._new_ids)
    except:
      print "run parse command first"
  
  def get_inserted(self): #{{{2
    try:
      self.inserted = self._new_ids.difference(self._old_ids)
    except:
      print "run parse command first"
  
  def get_changed(self):
    #prepare data for checking updates
    to_check = self._old_ids.intersection(self._new_ids)
    to_check = list(to_check)
    to_check.sort()
    

    slice_size = min_slice = 128
    max_slice = len(to_check)/20
    while len(to_check):
        slice = to_check[0:slice_size]
        print slice_size
        lookup_ids = []
        new_data = []
        for i in slice:
            d = self.imported_data[i]
            local_id = global_id = uri = ''
            if d['uri']: uri = d['uri']
            if d['local_id']: local_id = d['local_id']
            if d['global_id']: global_id = d['global_id']  
            new_data.append((uri, local_id, global_id))
            lookup_ids.append(i)


        c.execute("select uri, local_id, global_id from name_indices where data_source_id = %s and name_string_id in (%s)" % (options.source_id,  ",".join(map(lambda x: str(x),lookup_ids))))
        if sha.new(marshal.dumps(c.fetchall())).digest() == sha.new(marshal.dumps(tuple(new_data))).digest():
            to_check = to_check[slice_size:]
            if slice_size * 2 < max_slice:
                slice_size *= 2
        elif slice_size > min_slice:
            slice_size /= 2
        else:
            for i in slice:
                new_data = self.imported_data[i]
                c.execute("select uri, local_id, global_id from name_indices where data_source_id = %s and name_string_id = %s", (options.source_id, i))
                res = c.fetchone()
                if new_data != res[0][0]:
                    data = mysql_escape(new_data, i)
                    c.execute("update name_indices set uri = %(uri)s, global_id = %(global_id)s, local_id = %(local_id)s where data_source_id = %(data_source_id)s and name_string_id = %(name_string_id)s" % data)
            to_check = to_check[slice_size:]
    print "done"


  def db_delete(self):
    if self.deleted:
      delete_ids = ",".join(map(lambda x: str(x), self.deleted))
      self.db.cursor.execute("delete from name_indices where data_source_id = %s and name_string_id in (%s)", (options.source_id, delete_ids))

  def db_insert(self):
    if self.inserted:
      inserts = []
      for i in self.inserted:
        data = self.db.escape_data(self.imported_data[i])
        data['name_string_id'] = i
        data['data_source_id'] = options.source_id
        inserts.append("(%(data_source_id)s , %(name_string_id)s, %(uri)s, %(local_id)s, %(global_id)s, now(), now())" % data)
      self.db.cursor.execute("insert into name_indices (data_source_id, name_string_id, uri, local_id, global_id, created_at, updated_at) values %s" % ",".join(inserts))
 
  def db_update(self):
    pass

  def db_store_statistics(self):
    pass

  def _process_node(self):
    if self.reader.NodeType() == 1: #start of a tag
        if self.reader.Name() == "dwc:ScientificName":
            self._current_tag = "raw_name"    
        elif self.reader.Name() == "dc:source":
            self._current_tag = "uri"
        elif self.reader.Name() == "dc:identifier":
            self._current_tag = "local_id"
        elif self.reader.Name() == "dwc:GlobalUniqueIdentifier":
            self._current_tag = "global_id"
        else:
            self._current_tag = None
    elif self.reader.NodeType() == 15: #end of a tag
        if self.reader.Name() == "record":
            name_string_id = self._prepare_record()
            self._append_imported_data(name_string_id)
            self._record = self._reset_record()
    elif self.reader.NodeType() == 3 and self._current_tag: #text node
        self._record[self._current_tag] = self.reader.Value().strip()
        self._current_tag = None

  def _prepare_record(self):
      c = self.db.cursor
      c.execute("select id from name_strings where name = %(raw_name)s", self._record)
      name_string_id = c.fetchone()
      if not name_string_id:
          c.execute("insert into name_strings (name, created_at, updated_at) values (%(raw_name)s, now(), now())", self._record)
          c.execute("select last_insert_id()")
          name_string_id = c.fetchone()
      return (name_string_id[0])    
    
  def _append_imported_data(self, name_string_id):
    if self.imported_data.has_key(name_string_id):
      self.duplicates.append(name_string_id)
    self.imported_data[name_string_id] = self._record.copy()

  def _reset_record(self):
    return {'data_source_id': options.source_id}

#script part {{{1
i = Importer()
i.parse()
i.get_deleted()
i.get_inserted()
i.get_changed()
if (i.deleted or i.inserted or i.changed):
  i.db_delete()
  i.db_insert()
  i.db_update()
  i.db_store_statistics()

