#!/usr/bin/env python
import os
import sys
from urllib import urlopen
import libxml2
import MySQLdb
import yaml
import marshal
import sha
from optparse import OptionParser

opts = OptionParser()
opts.add_option("-e", "--environment", dest="environment", default="development",
                  help="Specifies the environment of the system (development|test|producton).")

opts.add_option("-d", "--data-url", dest="data_url",
                  help="Specifies url which contains data for harvesting.")

opts.add_option("-s", "--source-id", dest="source_id",
                  help="Identifier of the data_source in GNA database.")

(options, args) = opts.parse_args()

if not (options.data_url and options.source_id and type(int(options.source_id)) == type(1)):
    raise Exception("DATA_URL and SOURCE_ID has to be entered")

RECORD_TEMPLATE = {"data_source_id": options.source_id}
    

def connectDb():

    db_data = os.popen('erb ' + sys.path[0] + '/../../config/database.yml').read()
    db_conf =  yaml.load(db_data)[options.environment]
    if not db_conf.has_key('socket'):
        db_conf['socket'] = "/tmp/mysql.sock"

    try:
        conn = MySQLdb.connect (
            host = db_conf['host'],
            user = db_conf['username'],
            passwd = db_conf['password'],
            unix_socket = db_conf['socket'],
            db = db_conf['database'])
    except MySQLdb.Error, e:
        print "Error %d: %s" % (e.args[0], e.args[1])
        sys.exit (1)
    return conn

def prepare_record(c, record):
    c.execute("select id from name_strings where name = '%s'" % record["raw_name"])
    name_string_id = c.fetchone()
    if not name_string_id:
        c.execute("insert into name_strings (name) values ('%s')" % record["raw_name"])
        c.execute("select last_insert_id()")
        name_string_id = c.fetchone()
    return ( name_string_id[0], record)    

def mysql_escape(data, name_string_id):
    for key in data.keys():
      if data[key]:
        data[key] = MySQLdb.escape_string(str(data[key]))
      else:
        data[key] = ""
    data['data_source_id'] = options.source_id
    data['name_string_id'] = name_string_id
    for key in ['uri','local_id','global_id']:
      if data.has_key(key) and data[key] != "":
        pass
        data[key] = "'" + data[key] + "'"
      else:
        data[key] = 'null'
    print data
    return data



def insert_records(c, imported_data):
    c.execute("select name_string_id from name_indices where data_source_id = %s" % options.source_id)
    old_ids = set(map(lambda x: x[0], c.fetchall()))
    new_ids = set(imported_data.keys())
    to_delete = old_ids.difference(new_ids)
    to_insert = new_ids.difference(old_ids)
    to_check = old_ids.intersection(new_ids)
    if len(to_delete):
        delete_ids = ",".join(map(lambda x: str(x), to_delete))
        c.execute("delete from name_indices where data_source_id = %s and name_string_id in (%s)" % (options.source_id, delete_ids))
    if len(to_insert):
        inserts = []
        for i in to_insert:
            data = mysql_escape(imported_data[i], i)
            inserts.append("(%(data_source_id)s , %(name_string_id)s, %(uri)s, %(local_id)s, %(global_id)s)" % data)
        c.execute("insert into name_indices (data_source_id, name_string_id, uri, local_id, global_id) values %s" % ",".join(inserts))
    
    update_records(c, to_check, imported_data)
    
    print "deleted: " + str(len(to_delete)), "inserted: " + str(len(to_insert)), "checked for updates: " + str(len(to_check))

def update_records(c, to_check, imported_data):
    return
    to_check = list(to_check)
    to_check.sort()
    slice_size = min_slice = 128
    max_slice = len(to_check)/20
    while len(to_check):
        slice = to_check[0:slice_size]
        print slice_size
        lookup_ids = []
        new_data = []
        for i in slice:
            d = imported_dataa[i]
            local_id = global_id = uri = ''
            if d['uri']: uri = d['uri']
            if d['local_id']: local_id = d['local_id']
            if d['global_id']: global_id = d['global_id']  
            new_data.append((uri, local_id, global_id))
            lookup_ids.append(i)


        c.execute("select uri, local_id, global_id from name_indices where data_source_id = %s and name_string_id in (%s)" % (options.source_id,  ",".join(map(lambda x: str(x),lookup_ids))))
        if sha.new(marshal.dumps(c.fetchall())).digest() == sha.new(marshal.dumps(tuple(new_data))).digest():
            to_check = to_check[slice_size:]
            if slice_size * 2 < max_slice:
                slice_size *= 2
        elif slice_size > min_slice:
            slice_size /= 2
        else:
            for i in slice:
                new_data = imported_data[i]
                c.execute("select uri, local_id, global_id from name_indices where data_source_id = %s and name_string_id = %s", (options.source_id, i))
                res = c.fetchone()
                if new_data != res[0][0]:
                    data = mysql_escape(new_data, i)
                    c.execute("update name_indices set uri = %(uri)s, global_id = %(global_id)s, local_id = %(local_id)s where data_source_id = %(data_source_id)s and name_string_id = %(name_string_id)s" % data)
            to_check = to_check[slice_size:]
    print "done"
    

def processNode(c, reader, current_var, record, imported_data):
    if reader.NodeType() == 1:  #start of a tag
        if reader.Name() == "dwc:ScientificName":
            current_var = "raw_name"    
        elif reader.Name() == "dc:source":
            current_var = "uri"
        elif reader.Name() == "dc:identifier":
            current_var = "local_id"
        elif reader.Name() == "dwc:GlobalUniqueIdentifier":
            current_var = "global_id"
    elif reader.NodeType() == 15: #end of a tag
        if reader.Name() == "record":
            res = (prepare_record(c, record))
            imported_data[res[0]] = res[1]
            record = RECORD_TEMPLATE
    elif reader.NodeType() == 3 and current_var: #text node
        record[current_var] = reader.Value().strip()
        current_var = None
    return (current_var, record)
    
def streamFile(filename):
    try:
        reader = libxml2.newTextReaderFilename(filename)
    except:
        print "cannot open %s" % (filename)
        return
    db_connection = connectDb()
    c = db_connection.cursor()
    imported_data = {}
    current_var = None
    record = RECORD_TEMPLATE      
    ret = reader.Read()

    while ret == 1:
        current_var, record = processNode(c, reader, current_var, record, imported_data)
        ret = reader.Read()

    if ret != 0:
        print "%s : failed to parse" % (filename)
        sys.exit(0)
    insert_records(c, imported_data)
    db_connection.commit()

#f = 'http://betula.mbl.edu/index_fungorum_test/data.xml'
#f = sys.path[0] + '/../spec/fixtures/feeds/index_fungorum_short.xml'

streamFile(options.data_url)
